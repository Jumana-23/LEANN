{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever-msmarco\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever-msmarco. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Computing embeddings for 1 chunks using SentenceTransformer model 'facebook/contriever-msmarco'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.51it/s]\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever-msmarco\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever-msmarco. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Computing embeddings for 5 chunks using SentenceTransformer model 'facebook/contriever-msmarco'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M: 64 for level: 0\n",
      "INFO: Converting HNSW index to CSR-pruned format...\n",
      "Starting conversion: knowledge.index -> knowledge.csr.tmp\n",
      "[0.00s] Reading Index HNSW header...\n",
      "[0.00s]   Header read: d=768, ntotal=5\n",
      "[0.00s] Reading HNSW struct vectors...\n",
      "  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48\n",
      "[0.00s]   Read assign_probas (6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28\n",
      "[0.14s]   Read cum_nneighbor_per_level (7)\n",
      "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=5, Bytes=20\n",
      "[0.22s]   Read levels (5)\n",
      "[0.31s]   Probing for compact storage flag...\n",
      "[0.31s]   Found compact flag: False\n",
      "[0.31s]   Compact flag is False, reading original format...\n",
      "[0.31s]   Probing for potential extra byte before non-compact offsets...\n",
      "[0.31s]   Found and consumed an unexpected 0x00 byte.\n",
      "  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=6, Bytes=48\n",
      "[0.31s]   Read offsets (6)\n",
      "[0.39s]   Attempting to read neighbors vector...\n",
      "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=320, Bytes=1280\n",
      "[0.39s]   Read neighbors (320)\n",
      "[0.47s]   Read scalar params (ep=4, max_lvl=0)\n",
      "[0.47s] Checking for storage data...\n",
      "[0.47s]   Found storage fourcc: 49467849.\n",
      "[0.47s] Converting to CSR format...\n",
      "[0.47s]   Conversion loop finished.                        \n",
      "[0.47s] Running validation checks...\n",
      "    Checking total valid neighbor count...\n",
      "    OK: Total valid neighbors = 20\n",
      "    Checking final pointer indices...\n",
      "    OK: Final pointers match data size.\n",
      "[0.47s] Deleting original neighbors and offsets arrays...\n",
      "    CSR Stats: |data|=20, |level_ptr|=10\n",
      "[0.56s] Writing CSR HNSW graph data in FAISS-compatible order...\n",
      "   Pruning embeddings: Writing NULL storage marker.\n",
      "[0.64s] Conversion complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever-msmarco\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever-msmarco. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CSR conversion successful.\n",
      "INFO: Replaced original index with CSR-pruned version at 'knowledge.index'\n",
      "[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...\n",
      "[read_HNSW NL v4] Read levels vector, size: 5\n",
      "[read_HNSW NL v4] Reading Compact Storage format indices...\n",
      "[read_HNSW NL v4] Read compact_level_ptr, size: 10\n",
      "[read_HNSW NL v4] Read compact_node_offsets, size: 6\n",
      "[read_HNSW NL v4] Read entry_point: 4, max_level: 0\n",
      "[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e\n",
      "[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 326\n",
      "[read_HNSW NL v4] Reading neighbors data into memory.\n",
      "[read_HNSW NL v4] Read neighbors data, size: 20\n",
      "[read_HNSW NL v4] Finished reading metadata and CSR indices.\n",
      "INFO: Skipping external storage loading, since is_recompute is true.\n",
      "INFO: Terminating session server process (PID: 77679)...\n",
      "INFO: Server process terminated.\n",
      "üîç DEBUG LeannSearcher.search() called:\n",
      "  Query: 'programming languages'\n",
      "  Top_k: 2\n",
      "  Additional kwargs: {'recompute_beighbor_embeddings': True}\n",
      "INFO: Computing embeddings for 1 chunks using SentenceTransformer model 'facebook/contriever-msmarco'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated embedding shape: (1, 768)\n",
      "  Embedding time: 1.2287070751190186 seconds\n",
      "INFO: Starting session-level embedding server for 'leann_backend_hnsw.hnsw_embedding_server'...\n",
      "INFO: Running command from project root: /Users/yichuan/Desktop/code/LEANN/leann\n",
      "INFO: Command: /Users/yichuan/Desktop/code/LEANN/leann/.venv/bin/python -m leann_backend_hnsw.hnsw_embedding_server --zmq-port 5557 --model-name facebook/contriever-msmarco --passages-file knowledge.leann.meta.json --disable-warmup\n",
      "INFO: Server process started with PID: 77844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding server is up and ready for this session.\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: INFO: Starting backend auto-discovery...\n",
      "ZmqDistanceComputer initialized: d=768, metric=0\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: INFO: Registering backend 'diskann'\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: INFO: Backend auto-discovery finished.\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: INFO: Registering backend 'hnsw'\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Loading tokenizer for facebook/contriever-msmarco...\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Tokenizer loaded successfully!\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: MPS available: True\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: CUDA available: False\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Using MPS device (Apple Silicon)\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Starting HNSW server on port 5557 with model facebook/contriever-msmarco\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Loading model facebook/contriever-msmarco... (this may take a while if downloading)\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Model facebook/contriever-msmarco loaded successfully!\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Loaded label map with 5 entries\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Initialized lazy passage loading for 5 passages\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Warmup disabled or no passages available (enable_warmup=False, passages=5)\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: HNSW ZMQ server listening on port 5557\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Started HNSW ZMQ server thread on port 5557\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Received ZMQ request of size 3 bytes\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: request_payload length: 1\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: request_payload[0]: <class 'list'> - [4]\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Request for 1 node embeddings\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Total batch size: 1, max_batch_size: 128\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG zmq_server_thread: Final 'hidden' array | Shape: (1, 768) | Dtype: float32 | Has NaN/Inf: False\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Serialize time: 0.000268 seconds\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: ZMQ E2E time: 0.171174 seconds\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Received ZMQ request of size 3849 bytes\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: request_payload length: 2\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: request_payload[0]: <class 'list'> - [0, 1, 2, 3]\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: request_payload[1]: <class 'list'> - [0.028167724609375, -0.01134490966796875, 0.044586181640625, -0.017486572265625, -0.028564453125, -0...\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: Distance calculation request received\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Node IDs: [0, 1, 2, 3]\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Query vector dim: 768\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Passages loaded: 5\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: Looking up passage ID 0\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: Found text for ID 0, length: 64\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: Looking up passage ID 1\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: Found text for ID 1, length: 64\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: Looking up passage ID 2\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: Found text for ID 2, length: 38\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: Looking up passage ID 3\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: Found text for ID 3, length: 36\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Sending distance response with 4 distances\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Distance calculation E2E time: 0.143306 seconds\n",
      "  Search time: 5.966892957687378 seconds\n",
      "  Backend returned: labels=2 results\n",
      "  Processing 2 passage IDs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:leann.chat:Attempting to create LLM of type='ollama' with model='qwen3:8b'\n",
      "INFO:leann.chat:Initializing OllamaChat with model='qwen3:8b' and host='http://localhost:11434'\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever-msmarco\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1. passage_id='90d76803-a8a0-4693-ab89-0190a4dbce09' -> SUCCESS: Python is a powerful programming language and it is very popular...\n",
      "    2. passage_id='f831e915-3cca-4504-bda3-ee59c32fb151' -> SUCCESS: C# is a powerful programming language but it is not very popular...\n",
      "  Final enriched results: 2 passages\n",
      "[SearchResult(id='90d76803-a8a0-4693-ab89-0190a4dbce09', score=np.float32(1.394449), text='Python is a powerful programming language and it is very popular', metadata={}), SearchResult(id='f831e915-3cca-4504-bda3-ee59c32fb151', score=np.float32(1.3833004), text='C# is a powerful programming language but it is not very popular', metadata={})]\n",
      "[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...\n",
      "[read_HNSW NL v4] Read levels vector, size: 5\n",
      "[read_HNSW NL v4] Reading Compact Storage format indices...\n",
      "[read_HNSW NL v4] Read compact_level_ptr, size: 10\n",
      "[read_HNSW NL v4] Read compact_node_offsets, size: 6\n",
      "[read_HNSW NL v4] Read entry_point: 4, max_level: 0\n",
      "[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e\n",
      "[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 326\n",
      "[read_HNSW NL v4] Reading neighbors data into memory.\n",
      "[read_HNSW NL v4] Read neighbors data, size: 20\n",
      "[read_HNSW NL v4] Finished reading metadata and CSR indices.\n",
      "INFO: Skipping external storage loading, since is_recompute is true.\n",
      "üîç DEBUG LeannSearcher.search() called:\n",
      "  Query: 'Compare the two retrieved programming languages and say which one is more popular today. Respond in a single well-formed sentence.'\n",
      "  Top_k: 2\n",
      "  Additional kwargs: {'recompute_beighbor_embeddings': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever-msmarco. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Computing embeddings for 1 chunks using SentenceTransformer model 'facebook/contriever-msmarco'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated embedding shape: (1, 768)\n",
      "  Embedding time: 2.0163228511810303 seconds\n",
      "INFO: Port 5557 is in use. Checking server compatibility...\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Received ZMQ request of size 17 bytes\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: request_payload length: 1\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: request_payload[0]: <class 'str'> - __QUERY_MODEL__\n",
      "‚úÖ Existing server on port 5557 is using correct model: facebook/contriever-msmarco\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Received ZMQ request of size 21 bytes\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: request_payload length: 1\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: request_payload[0]: <class 'str'> - __QUERY_META_PATH__\n",
      "‚úÖ Existing server on port 5557 is using correct meta path: knowledge.leann.meta.json\n",
      "‚úÖ Server on port 5557 is compatible and ready to use.\n",
      "ZmqDistanceComputer initialized: d=768, metric=0\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Received ZMQ request of size 3 bytes\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: request_payload length: 1\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: request_payload[0]: <class 'list'> - [4]\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Request for 1 node embeddings\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Total batch size: 1, max_batch_size: 128\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG zmq_server_thread: Final 'hidden' array | Shape: (1, 768) | Dtype: float32 | Has NaN/Inf: False\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Serialize time: 0.000132 seconds\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: ZMQ E2E time: 0.075267 seconds\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Received ZMQ request of size 3849 bytes\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: request_payload length: 2\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: request_payload[0]: <class 'list'> - [0, 1, 2, 3]\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: request_payload[1]: <class 'list'> - [0.0753173828125, -0.00579071044921875, 0.0662841796875, 0.014923095703125, -0.0064544677734375, -0....\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: Distance calculation request received\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Node IDs: [0, 1, 2, 3]\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Query vector dim: 768\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Passages loaded: 5\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: Looking up passage ID 0\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: Found text for ID 0, length: 64\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: Looking up passage ID 1\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: Found text for ID 1, length: 64\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: Looking up passage ID 2\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: Found text for ID 2, length: 38\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: Looking up passage ID 3\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: DEBUG: Found text for ID 3, length: 36\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Sending distance response with 4 distances\n",
      "[leann_backend_hnsw.hnsw_embedding_server LOG]: Distance calculation E2E time: 0.066100 seconds\n",
      "  Search time: 0.14907574653625488 seconds\n",
      "  Backend returned: labels=2 results\n",
      "  Processing 2 passage IDs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:leann.chat:Sending request to Ollama: {'model': 'qwen3:8b', 'prompt': 'Here is some retrieved context that might help answer your question:\\n\\nPython is a powerful programming language and it is very popular\\n\\nC# is a powerful programming language but it is not very popular\\n\\nQuestion: Compare the two retrieved programming languages and say which one is more popular today. Respond in a single well-formed sentence.\\n\\nPlease provide the best answer you can based on this context and your knowledge.', 'stream': False, 'options': {}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1. passage_id='90d76803-a8a0-4693-ab89-0190a4dbce09' -> SUCCESS: Python is a powerful programming language and it is very popular...\n",
      "    2. passage_id='f831e915-3cca-4504-bda3-ee59c32fb151' -> SUCCESS: C# is a powerful programming language but it is not very popular...\n",
      "  Final enriched results: 2 passages\n",
      "<think>\n",
      "Okay, let's tackle this question. The user wants to compare Python and C# in terms of popularity today based on the provided context and my knowledge. \n",
      "\n",
      "First, looking at the retrieved context: it says Python is very popular, while C# is not very popular. That's a clear indicator from the given data. But I should also consider my existing knowledge to confirm.\n",
      "\n",
      "From what I know, Python has a massive community and is widely used in areas like data science, machine learning, web development, automation, and scripting. Its simplicity and readability make it a favorite for beginners and experts alike. Major companies like Google, Facebook, and Microsoft use Python extensively. The Python Package Index (PyPI) has thousands of libraries, which boosts its popularity.\n",
      "\n",
      "C#, on the other hand, is developed by Microsoft and is primarily used for Windows applications, game development with Unity, and enterprise software. While it's powerful and has a good ecosystem, its usage is more niche compared to Python. The .NET framework and Visual Studio support C#, but the community isn't as large or diverse as Python's. Although C# is popular in specific domains, it doesn't have the same widespread adoption across different industries.\n",
      "\n",
      "So combining the context provided and my knowledge, Python is definitely more popular today. The context already states that, and my understanding aligns with that. The answer should reflect that Python is more popular than C# based on both the given info and general trends.\n",
      "</think>\n",
      "\n",
      "Python is more popular today than C# due to its widespread use in diverse fields like data science, web development, and automation, whereas C# is primarily used in specific domains such as Windows applications and game development.\n"
     ]
    }
   ],
   "source": [
    "from leann.api import LeannBuilder, LeannSearcher, LeannChat\n",
    "# 1. Build index (no embeddings stored!)\n",
    "builder = LeannBuilder(backend_name=\"hnsw\")\n",
    "builder.add_text(\"C# is a powerful programming language but it is not very popular\")\n",
    "builder.add_text(\"Python is a powerful programming language and it is very popular\")\n",
    "builder.add_text(\"Machine learning transforms industries\")  \n",
    "builder.add_text(\"Neural networks process complex data\")\n",
    "builder.add_text(\"Leann is a great storage saving engine for RAG on your macbook\")\n",
    "builder.build_index(\"knowledge.leann\")\n",
    "# 2. Search with real-time embeddings\n",
    "searcher = LeannSearcher(\"knowledge.leann\")\n",
    "results = searcher.search(\"programming languages\", top_k=2, recompute_beighbor_embeddings=True)\n",
    "print(results)\n",
    "\n",
    "llm_config = {\"type\": \"ollama\", \"model\": \"qwen3:8b\"}\n",
    "\n",
    "chat = LeannChat(index_path=\"knowledge.leann\", llm_config=llm_config)\n",
    "\n",
    "response = chat.ask(\n",
    "    \"Compare the two retrieved programming languages and say which one is more popular today. Respond in a single well-formed sentence.\",\n",
    "    top_k=2,\n",
    "    recompute_beighbor_embeddings=True,\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
