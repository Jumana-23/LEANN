{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2a9cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from leann import LeannBuilder,LeannChat,LeannSearcher\n",
    "from pathlib import Path\n",
    "INDEX_PATH = str(Path(\"./\").resolve()/\"demo.leann\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "006f6750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing passages: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 10979.85chunk/s]\n",
      "Computing Ollama embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.01it/s]\n",
      "INFO:leann_backend_hnsw.hnsw_backend:INFO: Converting HNSW index to CSR-pruned format...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M: 64 for level: 0\n",
      "Starting conversion: /Users/jetblue23/Desktop/leann/leann/demo.index -> /Users/jetblue23/Desktop/leann/leann/demo.csr.tmp\n",
      "[0.00s] Reading Index HNSW header...\n",
      "[0.00s]   Header read: d=768, ntotal=2\n",
      "[0.00s] Reading HNSW struct vectors...\n",
      "  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48\n",
      "[0.00s]   Read assign_probas (6)\n",
      "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28\n",
      "[0.08s]   Read cum_nneighbor_per_level (7)\n",
      "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=2, Bytes=8\n",
      "[0.13s]   Read levels (2)\n",
      "[0.17s]   Probing for compact storage flag...\n",
      "[0.17s]   Found compact flag: False\n",
      "[0.17s]   Compact flag is False, reading original format...\n",
      "[0.17s]   Probing for potential extra byte before non-compact offsets...\n",
      "[0.17s]   Found and consumed an unexpected 0x00 byte.\n",
      "  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=3, Bytes=24\n",
      "[0.17s]   Read offsets (3)\n",
      "[0.21s]   Attempting to read neighbors vector...\n",
      "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=128, Bytes=512\n",
      "[0.21s]   Read neighbors (128)\n",
      "[0.25s]   Read scalar params (ep=1, max_lvl=0)\n",
      "[0.25s] Checking for storage data...\n",
      "[0.25s]   Found storage fourcc: 49467849.\n",
      "[0.25s] Converting to CSR format...\n",
      "[0.25s]   Conversion loop finished.                        \n",
      "[0.25s] Running validation checks...\n",
      "    Checking total valid neighbor count...\n",
      "    OK: Total valid neighbors = 2\n",
      "    Checking final pointer indices...\n",
      "    OK: Final pointers match data size.\n",
      "[0.25s] Deleting original neighbors and offsets arrays...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:leann_backend_hnsw.hnsw_backend:‚úÖ CSR conversion successful.\n",
      "INFO:leann_backend_hnsw.hnsw_backend:INFO: Replaced original index with CSR-pruned version at '/Users/jetblue23/Desktop/leann/leann/demo.index'\n",
      "INFO:leann.embedding_server_manager:Terminating server process (PID: 9145) for backend leann_backend_hnsw.hnsw_embedding_server...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    CSR Stats: |data|=2, |level_ptr|=4\n",
      "[0.28s] Writing CSR HNSW graph data in FAISS-compatible order...\n",
      "   Pruning embeddings: Writing NULL storage marker.\n",
      "[0.32s] Conversion complete.\n",
      " Index built with embeddinggemma:latest(Ollama)\n",
      "[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...\n",
      "[read_HNSW NL v4] Read levels vector, size: 2\n",
      "[read_HNSW NL v4] Reading Compact Storage format indices...\n",
      "[read_HNSW NL v4] Read compact_level_ptr, size: 4\n",
      "[read_HNSW NL v4] Read compact_node_offsets, size: 3\n",
      "[read_HNSW NL v4] Read entry_point: 1, max_level: 0\n",
      "[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e\n",
      "[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242\n",
      "[read_HNSW NL v4] Reading neighbors data into memory.\n",
      "[read_HNSW NL v4] Read neighbors data, size: 2\n",
      "[read_HNSW NL v4] Finished reading metadata and CSR indices.\n",
      "INFO: Skipping external storage loading, since is_recompute is true.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:leann.embedding_server_manager:Server process 9145 terminated gracefully.\n",
      "INFO:leann.embedding_server_manager:Server process 9145 cleanup completed\n",
      "INFO:leann.api:üîç LeannSearcher.search() called:\n",
      "INFO:leann.api:  Query: 'Fantastical AI-generated creatures'\n",
      "INFO:leann.api:  Top_k: 1\n",
      "INFO:leann.api:  Metadata filters: None\n",
      "INFO:leann.api:  Additional kwargs: {}\n",
      "INFO:leann.embedding_server_manager:Starting embedding server on port 5557...\n",
      "INFO:leann.embedding_server_manager:Command: /Users/jetblue23/Desktop/leann/leann/.venv/bin/python -m leann_backend_hnsw.hnsw_embedding_server --zmq-port 5557 --model-name embeddinggemma:latest --passages-file /Users/jetblue23/Desktop/leann/leann/demo.leann.meta.json --embedding-mode ollama --distance-metric mips\n",
      "INFO:leann.embedding_server_manager:Starting server process with command: /Users/jetblue23/Desktop/leann/leann/.venv/bin/python -m leann_backend_hnsw.hnsw_embedding_server --zmq-port 5557 --model-name embeddinggemma:latest --passages-file /Users/jetblue23/Desktop/leann/leann/demo.leann.meta.json --embedding-mode ollama --distance-metric mips\n",
      "INFO:leann.embedding_server_manager:Server process started with PID: 10261\n",
      "INFO:leann.embedding_server_manager:Embedding server is ready!\n",
      "INFO:leann.api:  Launching server time: 1.0243010520935059 seconds\n",
      "INFO:leann.embedding_server_manager:Reusing in-process server\n",
      "INFO:leann.api:  Generated embedding shape: (1, 768)\n",
      "INFO:leann.api:  Embedding time: 0.2008967399597168 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZmqDistanceComputer initialized: d=768, metric=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:leann_backend_hnsw.hnsw_backend:  Search time in HNSWSearcher.search() backend: 0.20774292945861816 seconds\n",
      "INFO:leann.api:  Search time in search() LEANN searcher: 0.20852208137512207 seconds\n",
      "INFO:leann.api:  Backend returned: labels=1 results\n",
      "INFO:leann.api:  Processing 1 passage IDs:\n",
      "INFO:leann.api:   \u001b[92m‚úì\u001b[0m \u001b[94m[ 1]\u001b[0m \u001b[93mID:\u001b[0m '1' \u001b[93mScore:\u001b[0m 0.7438 \u001b[93mText:\u001b[0m Gemini called they need their nano-banana model back\n",
      "INFO:leann.api:  \u001b[92m‚úì Final enriched results: 1 passages\u001b[0m\n",
      "INFO:leann.chat:Attempting to create LLM of type='ollama' with model='gpt-oss:20b'\n",
      "INFO:leann.chat:Initializing OllamaChat with model='gpt-oss:20b' and host='http://localhost:11434'\n",
      "INFO:leann.api:üîç LeannSearcher.search() called:\n",
      "INFO:leann.api:  Query: 'How much storage does LEANN save?'\n",
      "INFO:leann.api:  Top_k: 1\n",
      "INFO:leann.api:  Metadata filters: None\n",
      "INFO:leann.api:  Additional kwargs: {}\n",
      "INFO:leann.embedding_server_manager:Starting embedding server on port 5558...\n",
      "INFO:leann.embedding_server_manager:Command: /Users/jetblue23/Desktop/leann/leann/.venv/bin/python -m leann_backend_hnsw.hnsw_embedding_server --zmq-port 5558 --model-name embeddinggemma:latest --passages-file /Users/jetblue23/Desktop/leann/leann/demo.leann.meta.json --embedding-mode ollama --distance-metric mips\n",
      "INFO:leann.embedding_server_manager:Starting server process with command: /Users/jetblue23/Desktop/leann/leann/.venv/bin/python -m leann_backend_hnsw.hnsw_embedding_server --zmq-port 5558 --model-name embeddinggemma:latest --passages-file /Users/jetblue23/Desktop/leann/leann/demo.leann.meta.json --embedding-mode ollama --distance-metric mips\n",
      "INFO:leann.embedding_server_manager:Server process started with PID: 10273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search result: Gemini called they need their nano-banana model back\n",
      "[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...\n",
      "[read_HNSW NL v4] Read levels vector, size: 2\n",
      "[read_HNSW NL v4] Reading Compact Storage format indices...\n",
      "[read_HNSW NL v4] Read compact_level_ptr, size: 4\n",
      "[read_HNSW NL v4] Read compact_node_offsets, size: 3\n",
      "[read_HNSW NL v4] Read entry_point: 1, max_level: 0\n",
      "[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e\n",
      "[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242\n",
      "[read_HNSW NL v4] Reading neighbors data into memory.\n",
      "[read_HNSW NL v4] Read neighbors data, size: 2\n",
      "[read_HNSW NL v4] Finished reading metadata and CSR indices.\n",
      "INFO: Skipping external storage loading, since is_recompute is true.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:leann.embedding_server_manager:Embedding server is ready!\n",
      "INFO:leann.api:  Launching server time: 0.5162389278411865 seconds\n",
      "INFO:leann.embedding_server_manager:Reusing in-process server\n",
      "INFO:leann.api:  Generated embedding shape: (1, 768)\n",
      "INFO:leann.api:  Embedding time: 0.17192411422729492 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZmqDistanceComputer initialized: d=768, metric=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:leann_backend_hnsw.hnsw_backend:  Search time in HNSWSearcher.search() backend: 0.21129512786865234 seconds\n",
      "INFO:leann.api:  Search time in search() LEANN searcher: 0.2123880386352539 seconds\n",
      "INFO:leann.api:  Backend returned: labels=1 results\n",
      "INFO:leann.api:  Processing 1 passage IDs:\n",
      "INFO:leann.api:   \u001b[92m‚úì\u001b[0m \u001b[94m[ 1]\u001b[0m \u001b[93mID:\u001b[0m '0' \u001b[93mScore:\u001b[0m 0.7741 \u001b[93mText:\u001b[0m Leann saves 97 percent storage compared to traditional vector databases.\n",
      "INFO:leann.api:  \u001b[92m‚úì Final enriched results: 1 passages\u001b[0m\n",
      "INFO:leann.api:  Search time: 0.9064779281616211 seconds\n",
      "INFO:leann.chat:Sending request to Ollama and waiting for response...\n",
      "INFO:leann.api:  Ask time: 39.90999507904053 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: LeANN can cut storage usage by **97‚ÄØ%** compared with a conventional vector database.\n"
     ]
    }
   ],
   "source": [
    "#### STEP 1: BUILD INDEX with embeddinggemma:latest \n",
    "\n",
    "builder = LeannBuilder(\n",
    "    backend_name = \"hnsw\", \n",
    "    embedding_model = \"embeddinggemma:latest\", \n",
    "    embedding_mode =\"ollama\", \n",
    "    graph_degree=32, \n",
    "    build_complexity = 64\n",
    ")\n",
    "builder.add_text(\"Leann saves 97 percent storage compared to traditional vector databases.\")\n",
    "builder.add_text(\"Gemini called they need their nano-banana model back\")\n",
    "builder.build_index(INDEX_PATH)\n",
    "print(\" Index built with embeddinggemma:latest(Ollama)\")\n",
    "\n",
    "##### STEP 2: SEARCH - ALWAYS EXPECT A LIST \n",
    "searcher = LeannSearcher(\n",
    "    INDEX_PATH, \n",
    "    embedding_model =\"embeddinggemma:latest\", \n",
    "    embedding_mode=\"ollama\",\n",
    "    search_complexity=32\n",
    ")\n",
    "results = searcher.search(\"Fantastical AI-generated creatures\", top_k=1)\n",
    "\n",
    "print(\"Search result:\", results[0].text if results else \"No match\")\n",
    "\n",
    "chat = LeannChat(\n",
    "    INDEX_PATH, \n",
    "    llm_config = {\"type\": \"ollama\", \"model\":\"gpt-oss:20b\"}, \n",
    "    embedding_model = \"embeddinggemma:latest\", \n",
    "    embedding_mode=\"ollama\", \n",
    "    thinking_budget=\"medium\"\n",
    ")\n",
    "response = chat.ask(\"How much storage does LEANN save?\", top_k=1)\n",
    "print(\"Answer:\", response) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (leann-env)",
   "language": "python",
   "name": "leann-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
